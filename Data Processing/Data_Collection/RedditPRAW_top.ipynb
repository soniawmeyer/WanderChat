{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb38982",
   "metadata": {},
   "source": [
    "# Scrape Travel subreddits using PRAW "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a2280b",
   "metadata": {},
   "source": [
    "Code implemented from:\n",
    "- https://artificialcorner.com/how-to-easily-scrape-data-from-social-media-the-example-of-reddit-138d619edfa5\n",
    "- https://towardsdatascience.com/scraping-reddit-data-1c0af3040768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1af353",
   "metadata": {},
   "source": [
    "Reddit API Documentation https://www.reddit.com/dev/api/#GET_hot\n",
    "\n",
    "List of travel related subreddits https://www.reddit.com/r/travel/comments/1100hca/the_definitive_list_of_travel_subreddits_to_help/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3342d7d",
   "metadata": {},
   "source": [
    "### Rate limits\n",
    "\n",
    "Rate limits with PRAW https://praw.readthedocs.io/en/latest/getting_started/ratelimits.html\n",
    "- Usually several hundred requests per minute depending on endpoint\n",
    "\n",
    "Rate limits with requests \n",
    "https://www.reddit.com/r/redditdev/comments/14nbw6g/updated_rate_limits_going_into_effect_over_the/\n",
    "- 100 queries per minute per OAuth client id if you are using OAuth authentication \n",
    "- 10 queries per minute if you are not using OAuth authentication\n",
    "\n",
    "https://www.reddit.com/r/redditdev/comments/151vty4/reddit_api/\n",
    "- You get to make 100 API requests per minute if you're making an app. For scripts, the limit is still 60 per minute. One search API request can give you up to 100 results.\n",
    "\n",
    "https://www.reddit.com/r/redditdev/comments/145liwv/api_changes_and_personal_oauth/\n",
    "- Effective July 1, 2023, the rate for apps that require higher usage limits is $0.24 per 1K API calls (less than a dollar 1.00 per user / month for a typical Reddit third-party app).\n",
    "\n",
    "Reddit API Rules (archived) https://github.com/reddit-archive/reddit/wiki/API#rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9ea36-ae98-4074-9197-03d172346d49",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a6dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from praw import Reddit\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f2bb9-e236-4115-8287-649797b32fb2",
   "metadata": {},
   "source": [
    "## Sonia's Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "246de873-567a-471c-9d97-7fd3aeff67f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 18:59:12.476588: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-12 18:59:12.476612: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-12 18:59:12.476636: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-12 18:59:12.482872: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 18:59:15.480344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 18:59:15.480579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 18:59:15.532159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 18:59:15.532341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 18:59:15.532484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-12 18:59:15.532621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b2728-2649-4a24-9873-16cd294a66b0",
   "metadata": {},
   "source": [
    "# Reddit Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5837faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# public key identifier\n",
    "my_client_ID = ''\n",
    "\n",
    "# secret key (do not show)\n",
    "my_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f38dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate Reddit App\n",
    "auth = requests.auth.HTTPBasicAuth(my_client_ID, my_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d453e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in pw from text file\n",
    "with open('pw.txt', 'r') as f:\n",
    "    pw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd90fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login - initialize a dict to specify we log in with a password\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    # pass in username and pw\n",
    "    'username': '',\n",
    "    'password': pw\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1155cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID version of our API\n",
    "my_user_agent = {'User-Agent': 'TravelAPI/0.0.1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46efa68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authorized instance\n",
    "reddit_authorized = Reddit(client_id=my_client_ID,\n",
    "                                client_secret=my_secret,\n",
    "                                user_agent=my_user_agent,\n",
    "                                username=\"\",\n",
    "                                password=pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b1286b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dev - identifying subreddit attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932a9116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<praw.models.listing.generator.ListingGenerator object at 0x7f7022bea530>\n"
     ]
    }
   ],
   "source": [
    "# instance is returned used to access each post using next()\n",
    "# limit to 100 for now\n",
    "travel_subreddit = reddit_authorized.subreddit('travel').hot(limit=100)\n",
    "print(travel_subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0319331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'praw.models.reddit.submission.Submission'>\n"
     ]
    }
   ],
   "source": [
    "next_reddit = next(travel_subreddit)\n",
    "print(type(next_reddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "408d880c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STR_FIELD,\t__class__,\t__delattr__,\t__dict__,\t__dir__\n",
      "__doc__,\t__eq__,\t__format__,\t__ge__,\t__getattr__\n",
      "__getattribute__,\t__gt__,\t__hash__,\t__init__,\t__init_subclass__\n",
      "__le__,\t__lt__,\t__module__,\t__ne__,\t__new__\n",
      "__reduce__,\t__reduce_ex__,\t__repr__,\t__setattr__,\t__sizeof__\n",
      "__str__,\t__subclasshook__,\t__weakref__,\t_additional_fetch_params,\t_chunk\n",
      "_comments_by_id,\t_edit_experimental,\t_fetch,\t_fetch_data,\t_fetch_info\n",
      "_fetched,\t_kind,\t_reddit,\t_replace_richtext_links,\t_reset_attributes\n",
      "_safely_add_arguments,\t_url_parts,\t_vote,\tadd_fetch_param,\tall_awardings\n",
      "allow_live_comments,\tapproved_at_utc,\tapproved_by,\tarchived,\tauthor\n",
      "author_flair_background_color,\tauthor_flair_css_class,\tauthor_flair_richtext,\tauthor_flair_template_id,\tauthor_flair_text\n",
      "author_flair_text_color,\tauthor_flair_type,\tauthor_fullname,\tauthor_is_blocked,\tauthor_patreon_flair\n",
      "author_premium,\taward,\tawarders,\tbanned_at_utc,\tbanned_by\n",
      "can_gild,\tcan_mod_post,\tcategory,\tclear_vote,\tclicked\n",
      "comment_limit,\tcomment_sort,\tcomments,\tcontent_categories,\tcontest_mode\n",
      "created,\tcreated_utc,\tcrosspost,\tdelete,\tdisable_inbox_replies\n",
      "discussion_type,\tdistinguished,\tdomain,\tdowns,\tdownvote\n",
      "duplicates,\tedit,\tedited,\tenable_inbox_replies,\tflair\n",
      "fullname,\tgild,\tgilded,\tgildings,\thidden\n",
      "hide,\thide_score,\tid,\tid_from_url,\tis_created_from_ads_ui\n",
      "is_crosspostable,\tis_meta,\tis_original_content,\tis_reddit_media_domain,\tis_robot_indexable\n",
      "is_self,\tis_video,\tlikes,\tlink_flair_background_color,\tlink_flair_css_class\n",
      "link_flair_richtext,\tlink_flair_text,\tlink_flair_text_color,\tlink_flair_type,\tlocked\n",
      "mark_visited,\tmedia,\tmedia_embed,\tmedia_only,\tmod\n",
      "mod_note,\tmod_reason_by,\tmod_reason_title,\tmod_reports,\tname\n",
      "no_follow,\tnum_comments,\tnum_crossposts,\tnum_reports,\tover_18\n",
      "parent_whitelist_status,\tparse,\tpermalink,\tpinned,\tpwls\n",
      "quarantine,\tremoval_reason,\tremoved_by,\tremoved_by_category,\treply\n",
      "report,\treport_reasons,\tsave,\tsaved,\tscore\n",
      "secure_media,\tsecure_media_embed,\tselftext,\tselftext_html,\tsend_replies\n",
      "shortlink,\tspoiler,\tstickied,\tsubreddit,\tsubreddit_id\n",
      "subreddit_name_prefixed,\tsubreddit_subscribers,\tsubreddit_type,\tsuggested_sort,\tthumbnail\n",
      "thumbnail_height,\tthumbnail_width,\ttitle,\ttop_awarded_type,\ttotal_awards_received\n",
      "treatment_tags,\tunhide,\tunsave,\tups,\tupvote\n",
      "upvote_ratio,\turl,\tuser_reports,\tview_count,\tvisited\n",
      "whitelist_status,\twls\n"
     ]
    }
   ],
   "source": [
    "all_attributes = dir(next_reddit) \n",
    "\n",
    "# Helper function to print all the attributes\n",
    "def print_attributes_in_table(data, columns):\n",
    "    for i in range(0, len(data), columns):\n",
    "        print(',\\t'.join(data[i:i+columns]))\n",
    "\n",
    "# Run the function\n",
    "print_attributes_in_table(all_attributes, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e28a10-e98f-4c4e-aa05-3b5f6a108cc4",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "647a5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments_from_forest(submission):\n",
    "\n",
    "    all_comments = []\n",
    "\n",
    "    # Start iterating through each comment in the forest and get the content\n",
    "    submission.comments.replace_more(limit=0) # Flatten the tree\n",
    "    comments = submission.comments.list() # all the comments\n",
    "\n",
    "    for comment in comments:\n",
    "        all_comments.append(comment.body)\n",
    "\n",
    "    return all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db8a8b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_N_posts(topic_of_interest, N = 100, sleep_time = 60):\n",
    "\n",
    "    topic_of_interest = topic_of_interest.replace(' ', '')\n",
    "    final_list_of_dict = []\n",
    "    dict_result = {}\n",
    "\n",
    "    try:\n",
    "        submissions = reddit_authorized.subreddit(topic_of_interest).top(time_filter ='year', limit=N)\n",
    "        time.sleep(sleep_time)\n",
    "    except praw.exceptions.APIException as e:\n",
    "        if e.response.status_code == 429:\n",
    "            # Rate limit exceeded, wait for the specified duration\n",
    "            retry_after = int(e.response.headers['Retry-After'])\n",
    "            print(f\"Rate limit exceeded. Waiting for {retry_after} seconds...\")\n",
    "            time.sleep(retry_after)\n",
    "        else:\n",
    "            # Handle other API exceptions\n",
    "            print(f\"API Exception: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        time.sleep(60)\n",
    "    # submissions = reddit_authorized.subreddit(topic_of_interest).top(time_filter ='day', limit=N)\n",
    "    # https://www.reddit.com/dev/api/#GET_hot\n",
    "    #limit for top is 100, top 100 per day, removing duplicates\n",
    "    #decided to do hot daily, limit 1000\n",
    "        \n",
    "    \n",
    "    for submission in submissions:\n",
    "    # 11 cols\n",
    "        dict_result[\"title\"] = submission.title\n",
    "        dict_result[\"selftext\"] = submission.selftext\n",
    "        dict_result[\"creation_date\"] = dt.datetime.fromtimestamp(submission.created)\n",
    "        dict_result[\"id\"] = submission.id\n",
    "        dict_result[\"url\"] = submission.url\n",
    "        dict_result[\"upvote_ratio\"] = submission.upvote_ratio\n",
    "        dict_result[\"ups\"] = submission.ups\n",
    "        dict_result[\"downs\"] = submission.downs\n",
    "        dict_result[\"score\"] = submission.score\n",
    "        dict_result[\"link_flair_css_class\"] = submission.link_flair_css_class\n",
    "        dict_result[\"comments\"] = extract_comments_from_forest(submission)\n",
    "    \n",
    "        final_list_of_dict.append(dict_result)\n",
    "        dict_result = {}\n",
    "\n",
    "    # Create the dataframe\n",
    "    df = pd.DataFrame(final_list_of_dict)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98996967",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dev - test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebfa520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took a few minutes to extract posts from travel subreddit\n",
    "# ~10 mins to call less than 1000 posts\n",
    "travel_reddits_df = extract_top_N_posts('travel', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88265da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (100, 5) means 100 rows and 11 columns\n",
    "print(travel_reddits_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ffb517a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>score</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Passport Questions &amp; Issues Megathread (2023)</td>\n",
       "      <td>NOTE: October 2023 **If the US Government has ...</td>\n",
       "      <td>2023-01-01 12:56:19</td>\n",
       "      <td>100t75r</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/100t7...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>542</td>\n",
       "      <td>0</td>\n",
       "      <td>542</td>\n",
       "      <td>question</td>\n",
       "      <td>[SPRING BREAK RUSH HAS STARTED. AS OF TODAY PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. Department of State - \"Worldwide Caution\"</td>\n",
       "      <td>U.S. Department of State issued a new travel a...</td>\n",
       "      <td>2023-10-19 10:41:36</td>\n",
       "      <td>17bouw5</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/17bou...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>737</td>\n",
       "      <td>0</td>\n",
       "      <td>737</td>\n",
       "      <td>advice</td>\n",
       "      <td>[Yes. They are routinely issued when there is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We need to be more supportive of each other IR...</td>\n",
       "      <td>I’ve been traveling solo for a little bit and ...</td>\n",
       "      <td>2023-11-12 07:05:56</td>\n",
       "      <td>17tm34k</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/17tm3...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>advice</td>\n",
       "      <td>[I get tired of the \"I'm a traveller not a tou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is a place people overlook because it isn...</td>\n",
       "      <td>For me, it is the Akshardham [Temple](https://...</td>\n",
       "      <td>2023-11-11 19:11:22</td>\n",
       "      <td>17tb91s</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/17tb9...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>492</td>\n",
       "      <td>0</td>\n",
       "      <td>492</td>\n",
       "      <td>advice</td>\n",
       "      <td>[Not sure it's truly overlooked, but Herculane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I haven't flew since before 9/11, have some qu...</td>\n",
       "      <td>I'm going to have to take a plane soon, and it...</td>\n",
       "      <td>2023-11-12 04:14:01</td>\n",
       "      <td>17tj0yw</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/17tj0...</td>\n",
       "      <td>0.65</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>question</td>\n",
       "      <td>[The volume of liquid is limited because in 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0      Passport Questions & Issues Megathread (2023)   \n",
       "1     U.S. Department of State - \"Worldwide Caution\"   \n",
       "2  We need to be more supportive of each other IR...   \n",
       "3  What is a place people overlook because it isn...   \n",
       "4  I haven't flew since before 9/11, have some qu...   \n",
       "\n",
       "                                            selftext       creation_date  \\\n",
       "0  NOTE: October 2023 **If the US Government has ... 2023-01-01 12:56:19   \n",
       "1  U.S. Department of State issued a new travel a... 2023-10-19 10:41:36   \n",
       "2  I’ve been traveling solo for a little bit and ... 2023-11-12 07:05:56   \n",
       "3  For me, it is the Akshardham [Temple](https://... 2023-11-11 19:11:22   \n",
       "4  I'm going to have to take a plane soon, and it... 2023-11-12 04:14:01   \n",
       "\n",
       "        id                                                url  upvote_ratio  \\\n",
       "0  100t75r  https://www.reddit.com/r/travel/comments/100t7...          0.99   \n",
       "1  17bouw5  https://www.reddit.com/r/travel/comments/17bou...          0.94   \n",
       "2  17tm34k  https://www.reddit.com/r/travel/comments/17tm3...          0.88   \n",
       "3  17tb91s  https://www.reddit.com/r/travel/comments/17tb9...          0.94   \n",
       "4  17tj0yw  https://www.reddit.com/r/travel/comments/17tj0...          0.65   \n",
       "\n",
       "   ups  downs  score link_flair_css_class  \\\n",
       "0  542      0    542             question   \n",
       "1  737      0    737               advice   \n",
       "2  177      0    177               advice   \n",
       "3  492      0    492               advice   \n",
       "4   80      0     80             question   \n",
       "\n",
       "                                            comments  \n",
       "0  [SPRING BREAK RUSH HAS STARTED. AS OF TODAY PR...  \n",
       "1  [Yes. They are routinely issued when there is ...  \n",
       "2  [I get tired of the \"I'm a traveller not a tou...  \n",
       "3  [Not sure it's truly overlooked, but Herculane...  \n",
       "4  [The volume of liquid is limited because in 20...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(travel_reddits_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1ed1839-ed9e-479b-b748-6716143c3281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [SPRING BREAK RUSH HAS STARTED. AS OF TODAY PR...\n",
       "1     [Yes. They are routinely issued when there is ...\n",
       "2     [I get tired of the \"I'm a traveller not a tou...\n",
       "3     [Not sure it's truly overlooked, but Herculane...\n",
       "4     [Waiters in Paris weren’t rude at all. People ...\n",
       "                            ...                        \n",
       "95    [it's basically a translation sheet with a sta...\n",
       "96    [You’re right, they are certainly exaggerating...\n",
       "97    [> Also, I purchased an insurance while in the...\n",
       "98    [**Notice:** Are you asking about a layover or...\n",
       "99    [No.  There's nothing to see just outside the ...\n",
       "Name: comments, Length: 100, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(travel_reddits_df.comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c82f313-9d71-432d-a290-0e4e71a820c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(travel_reddits_df.comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbff513-ded9-4856-bedf-fa22930afd28",
   "metadata": {},
   "source": [
    "# Output Directory & Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31045817-a273-4b4d-b609-cb5af29a95b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/joeymeyer/970-evo-plus/Sonia/bertproj/reddit/reddit_output'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = 'reddit'\n",
    "output_dir = os.path.join(os.getcwd(), f'{source}_output')\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2652231e-5bf8-4373-9494-5a14ef98b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "860fabf3-43b6-478d-b287-a1c2128cc2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = pd.read_csv('travel_subreddits.csv')['travel_subreddits'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "438bd6cd-41a2-4263-8255-2f3b2383f1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.9\n",
      "30330\n"
     ]
    }
   ],
   "source": [
    "sleep_time = 2 #seconds #100 requests per hour, is 0.6 seconds between requests\n",
    "days_to_run = 1\n",
    "\n",
    "sub_N = 1000\n",
    "country_N = 50\n",
    "city_N = 20\n",
    "\n",
    "countries = [country for country in subreddits[27:56]]\n",
    "cities = [city for city in subreddits[57:]]\n",
    "subreddits = subreddits[0:26]\n",
    "\n",
    "total_requests = len(countries)*country_N + len(cities)*city_N + len(subreddits)*sub_N\n",
    "print(round(total_requests*sleep_time/60/60,1)) #approx time\n",
    "print(total_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37081f58-9824-4824-b517-28c6cb91d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wait until the other one finished, so as to not overload api request limit\n",
    "# wait_time = int(3.4*60*60)\n",
    "\n",
    "# # Create a tqdm progress bar\n",
    "# with tqdm(total=wait_time, desc=\"Processing\") as pbar:\n",
    "#     for i in range(wait_time):\n",
    "#         time.sleep(1) \n",
    "#         pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32157e-ecdb-450b-9100-29aa6178cdb6",
   "metadata": {},
   "source": [
    "# Loop through Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1b76d80-ed5a-41c3-95e8-9282af1e3d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, countries:  10%|███▌                               | 3/29 [02:52<19:29, 44.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for unitedstates: received 403 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, countries:  52%|█████████████████▌                | 15/29 [19:20<13:57, 59.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for russia: received 403 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, countries: 100%|██████████████████████████████████| 29/29 [37:27<00:00, 77.51s/it]\n",
      "Processing day 1, cities:  16%|█████▊                              | 23/144 [07:22<19:26,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for medina: received 404 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  24%|████████▊                           | 35/144 [11:11<32:16, 17.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for milan: received 403 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  28%|██████████                          | 40/144 [12:08<19:19, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for cancún: received 404 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  33%|███████████▊                        | 47/144 [14:16<19:09, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for halong: received 403 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  44%|███████████████▊                    | 63/144 [19:12<15:03, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for lisbon: received 403 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  44%|████████████████                    | 64/144 [19:14<11:16,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for dammam: received 403 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  45%|████████████████▎                   | 65/144 [19:17<08:38,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for penangisland: received 404 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  47%|█████████████████                   | 68/144 [19:33<07:17,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for zhuhai: received 404 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  57%|████████████████████▌               | 82/144 [24:34<11:40, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for hurghada: received 404 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  69%|████████████████████████▊           | 99/144 [28:02<10:42, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for krabi: received 403 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  75%|██████████████████████████▎        | 108/144 [31:35<12:34, 20.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for düsseldorf: received 404 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  83%|█████████████████████████████▏     | 120/144 [35:55<04:31, 11.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for beirut: received 404 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  89%|███████████████████████████████    | 128/144 [37:07<02:16,  8.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for montevideo: received 403 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  93%|████████████████████████████████▌  | 134/144 [38:58<03:03, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for accra: received 404 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities:  98%|██████████████████████████████████▎| 141/144 [39:54<00:24,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for palmademallorca: received 404 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing day 1, cities: 100%|███████████████████████████████████| 144/144 [40:36<00:00, 16.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for laspalms: Redirect to /subreddits/search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "day_count = 1\n",
    "list_of_df = []\n",
    "\n",
    "while day_count <= days_to_run:\n",
    "    \n",
    "    today = dt.date.today()\n",
    "    \n",
    "    #SUBREDDITS\n",
    "    for i in tqdm(range(len(subreddits)), desc=f\"Processing day {day_count}, subreddits\", ncols=100):\n",
    "        subreddit = subreddits[i]\n",
    "        try:\n",
    "            df = extract_top_N_posts(subreddit, sub_N, sleep_time)\n",
    "            df.insert(0, 'source', source)\n",
    "            df.insert(1, 'filename', f'{subreddit}_top_{sub_N}_{today}')\n",
    "            list_of_df.append(df)\n",
    "            joblib.dump(list_of_df, f'{output_dir}/safety_net.pkl')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for {subreddit}: {e}\")\n",
    "\n",
    "    #COUNTRIES\n",
    "    for i in tqdm(range(len(countries)), desc=f\"Processing day {day_count}, countries\", ncols=100):\n",
    "        subreddit = countries[i]\n",
    "        try:\n",
    "            df = extract_top_N_posts(subreddit, country_N, sleep_time)\n",
    "            df.insert(0, 'source', source)\n",
    "            df.insert(1, 'filename', f'{subreddit}_top_{country_N}_{today}')\n",
    "            list_of_df.append(df)\n",
    "            joblib.dump(list_of_df, f'{output_dir}/safety_net.pkl')\n",
    "            # df = joblib.load(f'{output_dir}/safety_net.pkl')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for {subreddit}: {e}\")\n",
    "\n",
    "    #CITIES\n",
    "    for i in tqdm(range(len(cities)), desc=f\"Processing day {day_count}, cities\", ncols=100):\n",
    "        subreddit = cities[i]\n",
    "        try:\n",
    "            df = extract_top_N_posts(subreddit, city_N, sleep_time)\n",
    "            df.insert(0, 'source', source)\n",
    "            df.insert(1, 'filename', f'{subreddit}_top_{city_N}_{today}')\n",
    "            list_of_df.append(df)\n",
    "            joblib.dump(list_of_df, f'{output_dir}/safety_net.pkl')\n",
    "            # df = joblib.load(f'{output_dir}/safety_net.pkl')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for {subreddit}: {e}\")\n",
    "\n",
    "    day_count += 1\n",
    "\n",
    "    # Concatenate DataFrames along rows (stack vertically)\n",
    "    df = pd.concat(list_of_df, axis=0)\n",
    "    # Reset the index of the concatenated DataFrame\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv(f\"{output_dir}/all_travel_top_{today}.csv\")\n",
    "    \n",
    "    # time.sleep(24*60*60 - N*sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d1fe70e-87ec-4523-9c42-306e28edbdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3593, 13)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79acae6-ccca-4644-9793-728acd6013b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_df = joblib.load(f'{output_dir}/safety_net.pkl')\n",
    "# print(len(list_of_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
