{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZ9B1t8wF4_u"
   },
   "source": [
    "# Instructions\n",
    "\n",
    "This code is largely based on Chris's code. We all need to use a consistent NLP process (that way we only need to explain one process in our report). I decided to include stopwords for more natural conversational language processing. You will need to manually modify how to import the data into the notebook, file paths, etc.\n",
    "\n",
    "This takes in raw text data and outputs processed data as a csv. This does not filter or upload to Chroma DB (next steps). For now, just save processed data on your own computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cH1jUyhMwdgE"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uMyhxY_-LurU",
    "outputId": "209caeff-5b45-4a45-87d3-d02e9e9d9182"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in ./venv/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in ./venv/lib/python3.10/site-packages (from jsonlines) (23.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yMkISM43mXlQ",
    "outputId": "cfe569d5-4649-4e7a-c6ae-b76278a79894"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/joeymeyer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/joeymeyer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/joeymeyer/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/joeymeyer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/joeymeyer/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import ast\n",
    "from langdetect import detect\n",
    "\n",
    "from summarizer import Summarizer\n",
    "import random\n",
    "\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjZiupOUwju-",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3LhFBs5d5wC",
    "outputId": "e227568d-03a2-48d7-a065-77aa50c088cc"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mS7sLbHWyO3"
   },
   "outputs": [],
   "source": [
    "# drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XC8T-UJHKsE8",
    "outputId": "60d1e461-d631-4364-e6e7-8eb4b1373c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/SJSU Classes/*2023 Fall DATA298A/BERT data\n",
      "bertopic_test.pkl   train_0.jsonl  train_2.jsonl  train_4.jsonl\n",
      "processed_data.csv  train_1.jsonl  train_3.jsonl  train_5.jsonl\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/'My Drive'/'SJSU Classes'/'*2023 Fall DATA298A'/'BERT data'\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvtviwyVLezW"
   },
   "outputs": [],
   "source": [
    "# Define the folder containing the JSONL files\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sonia's Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 18:04:53.772596: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-15 18:04:53.772625: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-15 18:04:53.772653: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-15 18:04:53.778772: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 18:04:56.879308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 18:04:56.879495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 18:04:56.913048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 18:04:56.913236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 18:04:56.913378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-15 18:04:56.913514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/joeymeyer/970-evo-plus/Sonia/bertproj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/joeymeyer/970-evo-plus/Sonia/bertproj/venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /media/joeymeyer/970-evo-plus/Sonia/bertproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34;42mbertproj\u001b[0m/  \u001b[34;42mdata_viz\u001b[0m/  \u001b[34;42mreddit\u001b[0m/  \u001b[34;42mvenv\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/joeymeyer/970-evo-plus/Sonia/bertproj'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-q5XE-_wqzD"
   },
   "source": [
    "## Get Data - UltraChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/joeymeyer/970-evo-plus/Sonia/bertproj/bertproj/ultrachat\n",
      "\u001b[0m\u001b[01;32mtrain_0.jsonl\u001b[0m*  \u001b[01;32mtrain_3.jsonl\u001b[0m*  \u001b[01;32mtrain_6.jsonl\u001b[0m*  \u001b[01;32mtrain_9.jsonl\u001b[0m*\n",
      "\u001b[01;32mtrain_1.jsonl\u001b[0m*  \u001b[01;32mtrain_4.jsonl\u001b[0m*  \u001b[01;32mtrain_7.jsonl\u001b[0m*  \u001b[34;42multrachat_1_output\u001b[0m/\n",
      "\u001b[01;32mtrain_2.jsonl\u001b[0m*  \u001b[01;32mtrain_5.jsonl\u001b[0m*  \u001b[01;32mtrain_8.jsonl\u001b[0m*  \u001b[34;42multrachat_rest_output\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%cd /media/joeymeyer/970-evo-plus/Sonia/bertproj/bertproj/ultrachat\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/joeymeyer/970-evo-plus/Sonia/bertproj/bertproj/ultrachat'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/joeymeyer/970-evo-plus/Sonia/bertproj/bertproj/ultrachat/ultrachat_rest_output'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = 'ultrachat_rest'\n",
    "output_dir = os.path.join(os.getcwd(), f'{source}_output')\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a count variable\n",
    "jsonl_count = 0\n",
    "\n",
    "# Iterate through the files in the directory\n",
    "for filename in os.listdir(cwd):\n",
    "    if filename.endswith(\".jsonl\"):\n",
    "        jsonl_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WUAm2l_-LPhF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 150000row [00:03, 42469.10row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for Reading JSON lines for 150000 conversations: 00:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# I used huggingface ultrachat data which is jsonlines file.\n",
    "# Manually update this to get data however you have it and in whatever format.\n",
    "\n",
    "with tqdm(total=jsonl_count, unit=\"row\", desc=\"Processing\", dynamic_ncols=True) as pbar: #pbar.update(1) ###\n",
    "    \n",
    "    # Initialize an empty list to store data from each JSONL file\n",
    "    data = []\n",
    "    \n",
    "    # Iterate through each JSONL file in the folder\n",
    "    for filename in os.listdir(cwd):\n",
    "        if filename.endswith(\".jsonl\"):\n",
    "            file_path = os.path.join(cwd, filename)\n",
    "    \n",
    "            # Open the JSONL file and read its contents\n",
    "            with jsonlines.open(file_path, 'r') as reader:\n",
    "                try:\n",
    "                    for item in reader:\n",
    "                        # Add the filename as a new key-value pair in each JSON object\n",
    "                        item['filename'] = filename ### MAKE SURE TO INCLUDE FILENAME\n",
    "                        data.append(item)\n",
    "                        pbar.update(1)\n",
    "                except jsonlines.InvalidLineError as e:\n",
    "                    print(f\"Skipped invalid line in {file_path}: {e}\")\n",
    "    \n",
    "            # break #remove eventually, just taking one file for testing\n",
    "    \n",
    "    # Convert the list of JSON objects into a Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    # Now you can work with the DataFrame 'df' containing data from all JSONL files\n",
    "\n",
    "# Update task with timing information\n",
    "task = f'Reading JSON lines for {df.shape[0]} conversations'\n",
    "print(f\"Elapsed time for {task}: {pbar.format_interval(pbar.last_print_t - pbar.start_t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>data</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[How can cross training benefit groups like ru...</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Are there any particular physical benefits to...</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[What percentage of the Earth's surface is cov...</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[How does language translation technology impa...</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[What is the most popular smartphone brand the...</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               data       filename\n",
       "0  0  [How can cross training benefit groups like ru...  train_0.jsonl\n",
       "1  1  [Are there any particular physical benefits to...  train_0.jsonl\n",
       "2  2  [What percentage of the Earth's surface is cov...  train_0.jsonl\n",
       "3  3  [How does language translation technology impa...  train_0.jsonl\n",
       "4  4  [What is the most popular smartphone brand the...  train_0.jsonl"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DjvvIIgLL_9U",
    "outputId": "4b6243a8-5df3-41d1-abed-bc48e83ece9c"
   },
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True)\n",
    "df['source'] = source\n",
    "df = df[df.columns[::-1]]\n",
    "\n",
    "# df = df.head() # here I'm just taking 5 for quick processing, you will want to do your whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rahJurmKWiFg",
    "outputId": "f849b1c4-5f49-4417-bc82-3b78acefc579"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>filename</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "      <td>[How can cross training benefit groups like ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "      <td>[Are there any particular physical benefits to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "      <td>[What percentage of the Earth's surface is cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "      <td>[How does language translation technology impa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "      <td>[What is the most popular smartphone brand the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319820</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_9.jsonl</td>\n",
       "      <td>[Could you please give me more information abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319821</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_9.jsonl</td>\n",
       "      <td>[Administer the Research Enhancement and Facul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319822</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_9.jsonl</td>\n",
       "      <td>[What is the purpose of the public viewing eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319823</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_9.jsonl</td>\n",
       "      <td>[Factory and headquarters of Foulds Macaroni C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319824</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_9.jsonl</td>\n",
       "      <td>[WHO WAS JESUS? WHAT IS THE TRINITY? AND WHY S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1319825 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            source       filename  \\\n",
       "0        ultrachat  train_0.jsonl   \n",
       "1        ultrachat  train_0.jsonl   \n",
       "2        ultrachat  train_0.jsonl   \n",
       "3        ultrachat  train_0.jsonl   \n",
       "4        ultrachat  train_0.jsonl   \n",
       "...            ...            ...   \n",
       "1319820  ultrachat  train_9.jsonl   \n",
       "1319821  ultrachat  train_9.jsonl   \n",
       "1319822  ultrachat  train_9.jsonl   \n",
       "1319823  ultrachat  train_9.jsonl   \n",
       "1319824  ultrachat  train_9.jsonl   \n",
       "\n",
       "                                                      data  \n",
       "0        [How can cross training benefit groups like ru...  \n",
       "1        [Are there any particular physical benefits to...  \n",
       "2        [What percentage of the Earth's surface is cov...  \n",
       "3        [How does language translation technology impa...  \n",
       "4        [What is the most popular smartphone brand the...  \n",
       "...                                                    ...  \n",
       "1319820  [Could you please give me more information abo...  \n",
       "1319821  [Administer the Research Enhancement and Facul...  \n",
       "1319822  [What is the purpose of the public viewing eve...  \n",
       "1319823  [Factory and headquarters of Foulds Macaroni C...  \n",
       "1319824  [WHO WAS JESUS? WHAT IS THE TRINITY? AND WHY S...  \n",
       "\n",
       "[1319825 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuIE6ERdS-Fe",
    "outputId": "3afbdee8-66ed-4f9b-ed25-85595d2b834c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3959475"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-q5XE-_wqzD"
   },
   "source": [
    "## Get Data - Dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/joeymeyer/970-evo-plus/Sonia/bertproj/bertproj/dolly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/joeymeyer/970-evo-plus/Sonia/bertproj/venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /media/joeymeyer/970-evo-plus/Sonia/bertproj/bertproj/dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mdatabricks-dolly-15k_travel.pkl\u001b[0m*  \u001b[34;42mdolly_output\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'dolly'\n",
    "output_dir = os.path.join(os.getcwd(),f'{source}_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice's parents have three daughters: Amy, Jes...</td>\n",
       "      <td></td>\n",
       "      <td>The name of the third daughter is Alice</td>\n",
       "      <td>open_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Given a reference text about Lollapalooza, whe...</td>\n",
       "      <td>Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an ann...</td>\n",
       "      <td>Lollapalooze is an annual musical festival hel...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is Thomas Jefferson?</td>\n",
       "      <td>Thomas Jefferson (April 13, 1743 – July 4, 182...</td>\n",
       "      <td>Thomas Jefferson (April 13, 1743 – July 4, 182...</td>\n",
       "      <td>information_extraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who was Kyle Van Zyl playing against when he s...</td>\n",
       "      <td>Van Zyl joined the Eastern Province Kings Acad...</td>\n",
       "      <td>Kyle Van Zyl was playing against Boland U21 wh...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0         When did Virgin Australia start operating?   \n",
       "1  Alice's parents have three daughters: Amy, Jes...   \n",
       "2  Given a reference text about Lollapalooza, whe...   \n",
       "3                           Who is Thomas Jefferson?   \n",
       "4  Who was Kyle Van Zyl playing against when he s...   \n",
       "\n",
       "                                             context  \\\n",
       "0  Virgin Australia, the trading name of Virgin A...   \n",
       "1                                                      \n",
       "2  Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an ann...   \n",
       "3  Thomas Jefferson (April 13, 1743 – July 4, 182...   \n",
       "4  Van Zyl joined the Eastern Province Kings Acad...   \n",
       "\n",
       "                                            response                category  \n",
       "0  Virgin Australia commenced services on 31 Augu...               closed_qa  \n",
       "1            The name of the third daughter is Alice                 open_qa  \n",
       "2  Lollapalooze is an annual musical festival hel...               closed_qa  \n",
       "3  Thomas Jefferson (April 13, 1743 – July 4, 182...  information_extraction  \n",
       "4  Kyle Van Zyl was playing against Boland U21 wh...               closed_qa  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Later, you can load the results\n",
    "with open('databricks-dolly-15k_travel.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)\n",
    "\n",
    "# Access the loaded results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined'] = df['instruction'].str.cat([df['context'], df['response']], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DjvvIIgLL_9U",
    "outputId": "4b6243a8-5df3-41d1-abed-bc48e83ece9c"
   },
   "outputs": [],
   "source": [
    "df['source'] = source\n",
    "df = df[df.columns[::-1]]\n",
    "\n",
    "# df = df.head() # here I'm just taking 5 for quick processing, you will want to do your whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rahJurmKWiFg",
    "outputId": "f849b1c4-5f49-4417-bc82-3b78acefc579"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>combined</th>\n",
       "      <th>category</th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dolly</td>\n",
       "      <td>When did Virgin Australia start operating? Vir...</td>\n",
       "      <td>closed_qa</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dolly</td>\n",
       "      <td>Alice's parents have three daughters: Amy, Jes...</td>\n",
       "      <td>open_qa</td>\n",
       "      <td>The name of the third daughter is Alice</td>\n",
       "      <td></td>\n",
       "      <td>Alice's parents have three daughters: Amy, Jes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dolly</td>\n",
       "      <td>Given a reference text about Lollapalooza, whe...</td>\n",
       "      <td>closed_qa</td>\n",
       "      <td>Lollapalooze is an annual musical festival hel...</td>\n",
       "      <td>Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an ann...</td>\n",
       "      <td>Given a reference text about Lollapalooza, whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dolly</td>\n",
       "      <td>Who is Thomas Jefferson? Thomas Jefferson (Apr...</td>\n",
       "      <td>information_extraction</td>\n",
       "      <td>Thomas Jefferson (April 13, 1743 – July 4, 182...</td>\n",
       "      <td>Thomas Jefferson (April 13, 1743 – July 4, 182...</td>\n",
       "      <td>Who is Thomas Jefferson?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dolly</td>\n",
       "      <td>Who was Kyle Van Zyl playing against when he s...</td>\n",
       "      <td>closed_qa</td>\n",
       "      <td>Kyle Van Zyl was playing against Boland U21 wh...</td>\n",
       "      <td>Van Zyl joined the Eastern Province Kings Acad...</td>\n",
       "      <td>Who was Kyle Van Zyl playing against when he s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>dolly</td>\n",
       "      <td>What is geomorphometry?  Geomorphometry, or ge...</td>\n",
       "      <td>open_qa</td>\n",
       "      <td>Geomorphometry, or geomorphometrics, is the sc...</td>\n",
       "      <td></td>\n",
       "      <td>What is geomorphometry?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5621</th>\n",
       "      <td>dolly</td>\n",
       "      <td>Tell me about the various road types in USA?  ...</td>\n",
       "      <td>classification</td>\n",
       "      <td>The public road system in United States of Ame...</td>\n",
       "      <td></td>\n",
       "      <td>Tell me about the various road types in USA?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622</th>\n",
       "      <td>dolly</td>\n",
       "      <td>What is a laser and who created it? A laser is...</td>\n",
       "      <td>summarization</td>\n",
       "      <td>A laser is a device that emits light from an e...</td>\n",
       "      <td>A laser is a device that emits light through a...</td>\n",
       "      <td>What is a laser and who created it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5623</th>\n",
       "      <td>dolly</td>\n",
       "      <td>How does GIS help in the real estate investmen...</td>\n",
       "      <td>general_qa</td>\n",
       "      <td>Real estate investors depend on precise, accur...</td>\n",
       "      <td></td>\n",
       "      <td>How does GIS help in the real estate investmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5624</th>\n",
       "      <td>dolly</td>\n",
       "      <td>What is the Masters?  The Masters Tournament i...</td>\n",
       "      <td>general_qa</td>\n",
       "      <td>The Masters Tournament is a golf tournament he...</td>\n",
       "      <td></td>\n",
       "      <td>What is the Masters?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5625 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     source                                           combined  \\\n",
       "0     dolly  When did Virgin Australia start operating? Vir...   \n",
       "1     dolly  Alice's parents have three daughters: Amy, Jes...   \n",
       "2     dolly  Given a reference text about Lollapalooza, whe...   \n",
       "3     dolly  Who is Thomas Jefferson? Thomas Jefferson (Apr...   \n",
       "4     dolly  Who was Kyle Van Zyl playing against when he s...   \n",
       "...     ...                                                ...   \n",
       "5620  dolly  What is geomorphometry?  Geomorphometry, or ge...   \n",
       "5621  dolly  Tell me about the various road types in USA?  ...   \n",
       "5622  dolly  What is a laser and who created it? A laser is...   \n",
       "5623  dolly  How does GIS help in the real estate investmen...   \n",
       "5624  dolly  What is the Masters?  The Masters Tournament i...   \n",
       "\n",
       "                    category  \\\n",
       "0                  closed_qa   \n",
       "1                    open_qa   \n",
       "2                  closed_qa   \n",
       "3     information_extraction   \n",
       "4                  closed_qa   \n",
       "...                      ...   \n",
       "5620                 open_qa   \n",
       "5621          classification   \n",
       "5622           summarization   \n",
       "5623              general_qa   \n",
       "5624              general_qa   \n",
       "\n",
       "                                               response  \\\n",
       "0     Virgin Australia commenced services on 31 Augu...   \n",
       "1               The name of the third daughter is Alice   \n",
       "2     Lollapalooze is an annual musical festival hel...   \n",
       "3     Thomas Jefferson (April 13, 1743 – July 4, 182...   \n",
       "4     Kyle Van Zyl was playing against Boland U21 wh...   \n",
       "...                                                 ...   \n",
       "5620  Geomorphometry, or geomorphometrics, is the sc...   \n",
       "5621  The public road system in United States of Ame...   \n",
       "5622  A laser is a device that emits light from an e...   \n",
       "5623  Real estate investors depend on precise, accur...   \n",
       "5624  The Masters Tournament is a golf tournament he...   \n",
       "\n",
       "                                                context  \\\n",
       "0     Virgin Australia, the trading name of Virgin A...   \n",
       "1                                                         \n",
       "2     Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an ann...   \n",
       "3     Thomas Jefferson (April 13, 1743 – July 4, 182...   \n",
       "4     Van Zyl joined the Eastern Province Kings Acad...   \n",
       "...                                                 ...   \n",
       "5620                                                      \n",
       "5621                                                      \n",
       "5622  A laser is a device that emits light through a...   \n",
       "5623                                                      \n",
       "5624                                                      \n",
       "\n",
       "                                            instruction  \n",
       "0            When did Virgin Australia start operating?  \n",
       "1     Alice's parents have three daughters: Amy, Jes...  \n",
       "2     Given a reference text about Lollapalooza, whe...  \n",
       "3                              Who is Thomas Jefferson?  \n",
       "4     Who was Kyle Van Zyl playing against when he s...  \n",
       "...                                                 ...  \n",
       "5620                            What is geomorphometry?  \n",
       "5621       Tell me about the various road types in USA?  \n",
       "5622                What is a laser and who created it?  \n",
       "5623  How does GIS help in the real estate investmen...  \n",
       "5624                               What is the Masters?  \n",
       "\n",
       "[5625 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuIE6ERdS-Fe",
    "outputId": "3afbdee8-66ed-4f9b-ed25-85595d2b834c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33750"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-q5XE-_wqzD"
   },
   "source": [
    "## Get Data - Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/joeymeyer/970-evo-plus/Sonia/bertproj/reddit/reddit_output/most_recent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/joeymeyer/970-evo-plus/Sonia/bertproj/venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /media/joeymeyer/970-evo-plus/Sonia/bertproj/reddit/reddit_output/most_recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mall_reddit_2023-11-12_to_2023-11-13.csv\u001b[0m*  \u001b[01;32mfinal_topic_results.pkl\u001b[0m*\n",
      "\u001b[01;32mfinal_bertopic_model.pkl\u001b[0m*                 \u001b[01;32mfinal_topic_visualization.png\u001b[0m*\n",
      "\u001b[01;32mfinal_heatmap_visualization.png\u001b[0m*          \u001b[01;32mprocessed_data.csv\u001b[0m*\n",
      "\u001b[01;32mfinal_hierarchy_visualization.png\u001b[0m*        \u001b[01;32mreddit_travel_df.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'reddit'\n",
    "output_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>score</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>reddit</td>\n",
       "      <td>travel...</td>\n",
       "      <td>Passpo...</td>\n",
       "      <td>NOTE: ...</td>\n",
       "      <td>2023-0...</td>\n",
       "      <td>100t75r</td>\n",
       "      <td>https:...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>536.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>question</td>\n",
       "      <td>['SPRI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>reddit</td>\n",
       "      <td>travel...</td>\n",
       "      <td>U.S. D...</td>\n",
       "      <td>U.S. D...</td>\n",
       "      <td>2023-1...</td>\n",
       "      <td>17bouw5</td>\n",
       "      <td>https:...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>743.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>743.0</td>\n",
       "      <td>advice</td>\n",
       "      <td>['Yes....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>reddit</td>\n",
       "      <td>travel...</td>\n",
       "      <td>[Updat...</td>\n",
       "      <td>First ...</td>\n",
       "      <td>2023-1...</td>\n",
       "      <td>17tvwy3</td>\n",
       "      <td>https:...</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1296.0</td>\n",
       "      <td>advice</td>\n",
       "      <td>['Whil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>reddit</td>\n",
       "      <td>travel...</td>\n",
       "      <td>Just m...</td>\n",
       "      <td>I’m Am...</td>\n",
       "      <td>2023-1...</td>\n",
       "      <td>17twyhu</td>\n",
       "      <td>https:...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>881.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>881.0</td>\n",
       "      <td>question</td>\n",
       "      <td>['Of c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>reddit</td>\n",
       "      <td>travel...</td>\n",
       "      <td>We nee...</td>\n",
       "      <td>I’ve b...</td>\n",
       "      <td>2023-1...</td>\n",
       "      <td>17tm34k</td>\n",
       "      <td>https:...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>383.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>advice</td>\n",
       "      <td>['I ge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  source   filename      title   selftext  \\\n",
       "0          0             0   reddit  travel...  Passpo...  NOTE: ...   \n",
       "1          1             1   reddit  travel...  U.S. D...  U.S. D...   \n",
       "2          2             2   reddit  travel...  [Updat...  First ...   \n",
       "3          3             3   reddit  travel...  Just m...  I’m Am...   \n",
       "4          4             4   reddit  travel...  We nee...  I’ve b...   \n",
       "\n",
       "  creation_date       id        url  upvote_ratio     ups  downs   score  \\\n",
       "0  2023-0...     100t75r  https:...       0.99      536.0    0.0   536.0   \n",
       "1  2023-1...     17bouw5  https:...       0.94      743.0    0.0   743.0   \n",
       "2  2023-1...     17tvwy3  https:...       0.96     1296.0    0.0  1296.0   \n",
       "3  2023-1...     17twyhu  https:...       0.88      881.0    0.0   881.0   \n",
       "4  2023-1...     17tm34k  https:...       0.91      383.0    0.0   383.0   \n",
       "\n",
       "  link_flair_css_class   comments  \n",
       "0   question            ['SPRI...  \n",
       "1     advice            ['Yes....  \n",
       "2     advice            ['Whil...  \n",
       "3   question            ['Of c...  \n",
       "4     advice            ['I ge...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('all_reddit_2023-11-12_to_2023-11-13.csv')\n",
    "\n",
    "pd.set_option('display.max_colwidth', 10)\n",
    "# Access the loaded results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9708, 15)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect the language of a text\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "df['language'] = df['title'].apply(detect_language)\n",
    "# Filter out rows with non-English text\n",
    "df = df[df['language'] == 'en']\n",
    "# Drop the 'language' column (optional)\n",
    "df = df.drop('language', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7210, 15)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['upvote_ratio'] >= 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5276, 15)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the strings back to lists using ast.literal_eval\n",
    "df['comments'] = df['comments'].apply(ast.literal_eval)\n",
    "df['comments'] = df['comments'].apply(lambda x: x[:20] if isinstance(x, list) else x)\n",
    "df = df[df['comments'].apply(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4313, 15)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(lst):\n",
    "    return '[' + ', '.join(map(str, lst)) + ']'\n",
    "\n",
    "# Apply the function to the 'comments' column\n",
    "df['comments'] = df['comments'].apply(lambda x: list_to_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2226"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['selftext'] = df['selftext'].replace('[nan]', np.nan)\n",
    "df['selftext'] = df['selftext'].replace('[]', np.nan)\n",
    "df['selftext'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.dropna(subset=['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to combine 'title', 'selftext', and 'comments' while skipping NaN values\n",
    "def combine_non_nan(row):\n",
    "    non_nan_values = [val for val in row if not pd.isna(val)]\n",
    "    return ' '.join(map(str, non_nan_values))\n",
    "\n",
    "# Apply the custom function to each row\n",
    "df['combined'] = df[['title', 'selftext', 'comments']].apply(combine_non_nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined'] = df['title'].str.cat([df['selftext'], df['comments']], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['combined'] = df['combined'].replace('[nan]', np.nan)\n",
    "df['combined'] = df['combined'].replace('[]', np.nan)\n",
    "df['combined'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rahJurmKWiFg",
    "outputId": "f849b1c4-5f49-4417-bc82-3b78acefc579"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>score</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>comments</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reddit</td>\n",
       "      <td>travel_hot_100_2023-11-12</td>\n",
       "      <td>Passport Questions &amp; Issues Megathread (2023)</td>\n",
       "      <td>NOTE: October 2023 **If the US Government has ...</td>\n",
       "      <td>2023-01-01 12:56:19</td>\n",
       "      <td>100t75r</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/100t7...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>536.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>question</td>\n",
       "      <td>[SPRING BREAK RUSH HAS STARTED. AS OF TODAY PR...</td>\n",
       "      <td>Passport Questions &amp; Issues Megathread (2023) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reddit</td>\n",
       "      <td>travel_hot_100_2023-11-12</td>\n",
       "      <td>U.S. Department of State - \"Worldwide Caution\"</td>\n",
       "      <td>U.S. Department of State issued a new travel a...</td>\n",
       "      <td>2023-10-19 10:41:36</td>\n",
       "      <td>17bouw5</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/17bou...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>743.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>743.0</td>\n",
       "      <td>advice</td>\n",
       "      <td>[Yes. They are routinely issued when there is ...</td>\n",
       "      <td>U.S. Department of State - \"Worldwide Caution\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reddit</td>\n",
       "      <td>travel_hot_100_2023-11-12</td>\n",
       "      <td>[Update] Jewelry stolen from luggage</td>\n",
       "      <td>First of all, I want to thank everyone who too...</td>\n",
       "      <td>2023-11-12 14:40:44</td>\n",
       "      <td>17tvwy3</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/17tvw...</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1296.0</td>\n",
       "      <td>advice</td>\n",
       "      <td>[While I’m glad you’re receiving full compensa...</td>\n",
       "      <td>[Update] Jewelry stolen from luggage First of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reddit</td>\n",
       "      <td>travel_hot_100_2023-11-12</td>\n",
       "      <td>Just me or is the US now far and away the most...</td>\n",
       "      <td>I’m American and everything from hotel prices/...</td>\n",
       "      <td>2023-11-12 15:27:10</td>\n",
       "      <td>17twyhu</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/17twy...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>881.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>881.0</td>\n",
       "      <td>question</td>\n",
       "      <td>[Of course. I live in San Diego and it blows m...</td>\n",
       "      <td>Just me or is the US now far and away the most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reddit</td>\n",
       "      <td>travel_hot_100_2023-11-12</td>\n",
       "      <td>We need to be more supportive of each other IR...</td>\n",
       "      <td>I’ve been traveling solo for a little bit and ...</td>\n",
       "      <td>2023-11-12 07:05:56</td>\n",
       "      <td>17tm34k</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/17tm3...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>383.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>advice</td>\n",
       "      <td>[I get tired of the \"I'm a traveller not a tou...</td>\n",
       "      <td>We need to be more supportive of each other IR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4308</th>\n",
       "      <td>reddit</td>\n",
       "      <td>adventures_top_1000_2023-11-12</td>\n",
       "      <td>Best city and itinerary in Southeast Asia for ...</td>\n",
       "      <td>I (18M from USA) am looking to head to Southea...</td>\n",
       "      <td>2023-02-01 20:21:32</td>\n",
       "      <td>10rgjc9</td>\n",
       "      <td>https://www.reddit.com/r/adventures/comments/1...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[SE Asia is super easy to get around, so I rec...</td>\n",
       "      <td>Best city and itinerary in Southeast Asia for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4309</th>\n",
       "      <td>reddit</td>\n",
       "      <td>adventures_top_1000_2023-11-12</td>\n",
       "      <td>Riverboarding Adventure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-26 13:58:54</td>\n",
       "      <td>10m3skn</td>\n",
       "      <td>https://youtu.be/qVti_oKTURY</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Riverboarding in the summer of 2022 on the Lo...</td>\n",
       "      <td>Riverboarding Adventure [Riverboarding in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4310</th>\n",
       "      <td>reddit</td>\n",
       "      <td>adventures_top_1000_2023-11-12</td>\n",
       "      <td>Walkabout ideas?</td>\n",
       "      <td>Recent life events have left me needing a soci...</td>\n",
       "      <td>2023-01-22 07:34:05</td>\n",
       "      <td>10imakx</td>\n",
       "      <td>https://www.reddit.com/r/adventures/comments/1...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[I'm 42 and enjoy solo backcountry hiking as w...</td>\n",
       "      <td>Walkabout ideas? Recent life events have left ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4311</th>\n",
       "      <td>reddit</td>\n",
       "      <td>adventures_top_1000_2023-11-12</td>\n",
       "      <td>Riverside Camping on Algoma's Whiskey Lake Loop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-22 06:26:40</td>\n",
       "      <td>10iku8b</td>\n",
       "      <td>https://youtu.be/wkDCh09Zcfc</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Spent the night fishing, cooking and having s...</td>\n",
       "      <td>Riverside Camping on Algoma's Whiskey Lake Loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4312</th>\n",
       "      <td>reddit</td>\n",
       "      <td>adventures_top_1000_2023-11-12</td>\n",
       "      <td>Winter Wild Camping at Kinder Down - Peak Dist...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-27 11:15:59</td>\n",
       "      <td>zwnguy</td>\n",
       "      <td>https://youtube.com/watch?v=__B_GP25Qpo&amp;featur...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hope you don't mind me sharing my first prope...</td>\n",
       "      <td>Winter Wild Camping at Kinder Down - Peak Dist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4313 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                        filename  \\\n",
       "0     reddit       travel_hot_100_2023-11-12   \n",
       "1     reddit       travel_hot_100_2023-11-12   \n",
       "2     reddit       travel_hot_100_2023-11-12   \n",
       "3     reddit       travel_hot_100_2023-11-12   \n",
       "4     reddit       travel_hot_100_2023-11-12   \n",
       "...      ...                             ...   \n",
       "4308  reddit  adventures_top_1000_2023-11-12   \n",
       "4309  reddit  adventures_top_1000_2023-11-12   \n",
       "4310  reddit  adventures_top_1000_2023-11-12   \n",
       "4311  reddit  adventures_top_1000_2023-11-12   \n",
       "4312  reddit  adventures_top_1000_2023-11-12   \n",
       "\n",
       "                                                  title  \\\n",
       "0         Passport Questions & Issues Megathread (2023)   \n",
       "1        U.S. Department of State - \"Worldwide Caution\"   \n",
       "2                  [Update] Jewelry stolen from luggage   \n",
       "3     Just me or is the US now far and away the most...   \n",
       "4     We need to be more supportive of each other IR...   \n",
       "...                                                 ...   \n",
       "4308  Best city and itinerary in Southeast Asia for ...   \n",
       "4309                            Riverboarding Adventure   \n",
       "4310                                   Walkabout ideas?   \n",
       "4311    Riverside Camping on Algoma's Whiskey Lake Loop   \n",
       "4312  Winter Wild Camping at Kinder Down - Peak Dist...   \n",
       "\n",
       "                                               selftext        creation_date  \\\n",
       "0     NOTE: October 2023 **If the US Government has ...  2023-01-01 12:56:19   \n",
       "1     U.S. Department of State issued a new travel a...  2023-10-19 10:41:36   \n",
       "2     First of all, I want to thank everyone who too...  2023-11-12 14:40:44   \n",
       "3     I’m American and everything from hotel prices/...  2023-11-12 15:27:10   \n",
       "4     I’ve been traveling solo for a little bit and ...  2023-11-12 07:05:56   \n",
       "...                                                 ...                  ...   \n",
       "4308  I (18M from USA) am looking to head to Southea...  2023-02-01 20:21:32   \n",
       "4309                                                NaN  2023-01-26 13:58:54   \n",
       "4310  Recent life events have left me needing a soci...  2023-01-22 07:34:05   \n",
       "4311                                                NaN  2023-01-22 06:26:40   \n",
       "4312                                                NaN  2022-12-27 11:15:59   \n",
       "\n",
       "           id                                                url  \\\n",
       "0     100t75r  https://www.reddit.com/r/travel/comments/100t7...   \n",
       "1     17bouw5  https://www.reddit.com/r/travel/comments/17bou...   \n",
       "2     17tvwy3  https://www.reddit.com/r/travel/comments/17tvw...   \n",
       "3     17twyhu  https://www.reddit.com/r/travel/comments/17twy...   \n",
       "4     17tm34k  https://www.reddit.com/r/travel/comments/17tm3...   \n",
       "...       ...                                                ...   \n",
       "4308  10rgjc9  https://www.reddit.com/r/adventures/comments/1...   \n",
       "4309  10m3skn                       https://youtu.be/qVti_oKTURY   \n",
       "4310  10imakx  https://www.reddit.com/r/adventures/comments/1...   \n",
       "4311  10iku8b                       https://youtu.be/wkDCh09Zcfc   \n",
       "4312   zwnguy  https://youtube.com/watch?v=__B_GP25Qpo&featur...   \n",
       "\n",
       "      upvote_ratio     ups  downs   score link_flair_css_class  \\\n",
       "0             0.99   536.0    0.0   536.0             question   \n",
       "1             0.94   743.0    0.0   743.0               advice   \n",
       "2             0.96  1296.0    0.0  1296.0               advice   \n",
       "3             0.88   881.0    0.0   881.0             question   \n",
       "4             0.91   383.0    0.0   383.0               advice   \n",
       "...            ...     ...    ...     ...                  ...   \n",
       "4308          1.00     1.0    0.0     1.0                  NaN   \n",
       "4309          1.00     1.0    0.0     1.0                  NaN   \n",
       "4310          1.00     1.0    0.0     1.0                  NaN   \n",
       "4311          1.00     1.0    0.0     1.0                  NaN   \n",
       "4312          1.00     1.0    0.0     1.0                  NaN   \n",
       "\n",
       "                                               comments  \\\n",
       "0     [SPRING BREAK RUSH HAS STARTED. AS OF TODAY PR...   \n",
       "1     [Yes. They are routinely issued when there is ...   \n",
       "2     [While I’m glad you’re receiving full compensa...   \n",
       "3     [Of course. I live in San Diego and it blows m...   \n",
       "4     [I get tired of the \"I'm a traveller not a tou...   \n",
       "...                                                 ...   \n",
       "4308  [SE Asia is super easy to get around, so I rec...   \n",
       "4309  [Riverboarding in the summer of 2022 on the Lo...   \n",
       "4310  [I'm 42 and enjoy solo backcountry hiking as w...   \n",
       "4311  [Spent the night fishing, cooking and having s...   \n",
       "4312  [Hope you don't mind me sharing my first prope...   \n",
       "\n",
       "                                               combined  \n",
       "0     Passport Questions & Issues Megathread (2023) ...  \n",
       "1     U.S. Department of State - \"Worldwide Caution\"...  \n",
       "2     [Update] Jewelry stolen from luggage First of ...  \n",
       "3     Just me or is the US now far and away the most...  \n",
       "4     We need to be more supportive of each other IR...  \n",
       "...                                                 ...  \n",
       "4308  Best city and itinerary in Southeast Asia for ...  \n",
       "4309  Riverboarding Adventure [Riverboarding in the ...  \n",
       "4310  Walkabout ideas? Recent life events have left ...  \n",
       "4311  Riverside Camping on Algoma's Whiskey Lake Loo...  \n",
       "4312  Winter Wild Camping at Kinder Down - Peak Dist...  \n",
       "\n",
       "[4313 rows x 14 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['Unnamed: 0.1','Unnamed: 0'])\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuIE6ERdS-Fe",
    "outputId": "3afbdee8-66ed-4f9b-ed25-85595d2b834c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4313, 14)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ImrIIPRwsk6"
   },
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true,
    "id": "UYok3abfmc06",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text=text.strip()\n",
    "    text=re.compile('<.*?>').sub('', text)\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    #text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    #text = re.sub(r'\\d',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text\n",
    "\n",
    "# # STOPWORD REMOVAL\n",
    "# def stopword(string):\n",
    "#     a= [i for i in string.split() if i not in STOPWORDS]\n",
    "#     return ' '.join(a)\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(preprocess(string)).replace('\\n','')\n",
    "    # return lemmatizer(stopword(preprocess(string)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27O6iKEzw1Xk"
   },
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hf1_WrD4QSSF",
    "outputId": "d40235cf-4cbd-4a72-890d-d7a311fc41af"
   },
   "outputs": [],
   "source": [
    "# Function to join the sentences in a list into a single string\n",
    "def combine_sentences(sentence_list):\n",
    "    return ' '.join(sentence_list)\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "df['combined'] = df['data'].apply(lambda x: combine_sentences(x)) #manually update column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee7ead3ba364141b5fee9f7d65bb1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|                                                        | 0/150000 [00:00<?, ?row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for NLP preprocessing for 150000 rows of text: 1:07:32\n"
     ]
    }
   ],
   "source": [
    "# #batch processing saved 1 min\n",
    "# #may be useful if other datasets are too large and hit RAM issues\n",
    "# #otherwise leave it\n",
    "\n",
    "# # Define batch size\n",
    "# batch_size = 30000\n",
    "\n",
    "# # Calculate the number of batches\n",
    "# num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "# batch_results = []\n",
    "\n",
    "# # Create a single progress bar outside the loop\n",
    "# with tqdm_notebook(total=len(df), unit=\"row\", desc=\"Processing\", dynamic_ncols=True) as pbar:\n",
    "#     # Process data in batches\n",
    "#     for i in range(num_batches): \n",
    "#         start_idx = i * batch_size\n",
    "#         end_idx = min((i + 1) * batch_size, len(df))\n",
    "#         batch_df = df.iloc[start_idx:end_idx]\n",
    "\n",
    "#         results = batch_df['combined'].apply(finalpreprocess)        \n",
    "#         batch_results.append(results)\n",
    "#         # Access and use the results as needed for each batch\n",
    "#         pbar.update(len(batch_df))  # Update the progress bar based on the batch size\n",
    "\n",
    "# elapsed_time = pbar.format_interval(pbar.last_print_t - pbar.start_t)\n",
    "\n",
    "# task = f'NLP preprocessing for {df.shape[0]} rows of text'\n",
    "# print(f\"Elapsed time for {task}: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>filename</th>\n",
       "      <th>data</th>\n",
       "      <th>combined</th>\n",
       "      <th>cause_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "      <td>[How can cross training benefit groups like ru...</td>\n",
       "      <td>How can cross training benefit groups like run...</td>\n",
       "      <td>how can cross training benefit group like runn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "      <td>[Are there any particular physical benefits to...</td>\n",
       "      <td>Are there any particular physical benefits to ...</td>\n",
       "      <td>be there any particular physical benefit to mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "      <td>[What percentage of the Earth's surface is cov...</td>\n",
       "      <td>What percentage of the Earth's surface is cove...</td>\n",
       "      <td>what percentage of the earth s surface be cove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "      <td>[How does language translation technology impa...</td>\n",
       "      <td>How does language translation technology impac...</td>\n",
       "      <td>how do language translation technology impact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ultrachat</td>\n",
       "      <td>train_0.jsonl</td>\n",
       "      <td>[What is the most popular smartphone brand the...</td>\n",
       "      <td>What is the most popular smartphone brand thes...</td>\n",
       "      <td>what be the most popular smartphone brand thes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source       filename  \\\n",
       "0  ultrachat  train_0.jsonl   \n",
       "1  ultrachat  train_0.jsonl   \n",
       "2  ultrachat  train_0.jsonl   \n",
       "3  ultrachat  train_0.jsonl   \n",
       "4  ultrachat  train_0.jsonl   \n",
       "\n",
       "                                                data  \\\n",
       "0  [How can cross training benefit groups like ru...   \n",
       "1  [Are there any particular physical benefits to...   \n",
       "2  [What percentage of the Earth's surface is cov...   \n",
       "3  [How does language translation technology impa...   \n",
       "4  [What is the most popular smartphone brand the...   \n",
       "\n",
       "                                            combined  \\\n",
       "0  How can cross training benefit groups like run...   \n",
       "1  Are there any particular physical benefits to ...   \n",
       "2  What percentage of the Earth's surface is cove...   \n",
       "3  How does language translation technology impac...   \n",
       "4  What is the most popular smartphone brand thes...   \n",
       "\n",
       "                                         cause_clean  \n",
       "0  how can cross training benefit group like runn...  \n",
       "1  be there any particular physical benefit to mi...  \n",
       "2  what percentage of the earth s surface be cove...  \n",
       "3  how do language translation technology impact ...  \n",
       "4  what be the most popular smartphone brand thes...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results_series = pd.concat(batch_results, axis=0)\n",
    "# df['cause_clean'] = results_series\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "0cJmbKC9mqJe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████| 4313/4313 [01:24<00:00, 51.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for NLP preprocessing for 4313 rows of text: 1.40 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "tqdm.pandas(desc=\"Processing\")\n",
    "df['cause_clean'] = df['combined'].progress_apply(finalpreprocess)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "#update task\n",
    "task = f'NLP preprocessing for {df.shape[0]} rows of text'\n",
    "if elapsed_time < 60:\n",
    "    print(f\"Elapsed time for {task}: {elapsed_time:.1f} seconds\")\n",
    "elif elapsed_time < 60 * 10:\n",
    "    print(f\"Elapsed time for {task}: {elapsed_time/60:.2f} minutes\")\n",
    "else:\n",
    "    print(f\"Elapsed time for {task}: {elapsed_time/60/60:.2f} hours\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('processed_data.csv')\n",
    "# df = df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_sample(text, model):\n",
    "    words = text.split()\n",
    "    max_size = len(words)\n",
    "    sample_size = 100\n",
    "    if sample_size > max_size:\n",
    "        return text\n",
    "    else:\n",
    "        random_sample = \" \".join(random.sample(words, sample_size))\n",
    "        summary = model(random_sample, ratio=0.2)\n",
    "        if summary != \"\":\n",
    "            return summary\n",
    "        else:\n",
    "            random_sample = \" \".join(random.sample(words, 75))\n",
    "            summary = model(random_sample, ratio=0.2)\n",
    "            return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████| 4313/4313 [01:31<00:00, 46.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for NLP preprocessing for 4313 rows of text: 1.59 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model = Summarizer()\n",
    "\n",
    "tqdm.pandas(desc=\"Processing\")\n",
    "df['bert_summary'] = df['cause_clean'].progress_apply(lambda x: summarize_sample(x, model))\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "#update task\n",
    "task = f'NLP preprocessing for {df.shape[0]} rows of text'\n",
    "if elapsed_time < 60:\n",
    "    print(f\"Elapsed time for {task}: {elapsed_time:.1f} seconds\")\n",
    "elif elapsed_time < 60 * 60:\n",
    "    print(f\"Elapsed time for {task}: {elapsed_time/60:.2f} minutes\")\n",
    "else:\n",
    "    print(f\"Elapsed time for {task}: {elapsed_time/60/60:.2f} hours\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bert_summary'].replace('', np.nan, inplace=True)\n",
    "round(df['bert_summary'].isnull().sum()/df.shape[0],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHECK HERE TO SEE THAT NULL ISN'T TOO HIGH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4313, 16)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4312, 16)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['bert_summary'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>score</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>comments</th>\n",
       "      <th>combined</th>\n",
       "      <th>cause_clean</th>\n",
       "      <th>bert_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reddit</td>\n",
       "      <td>travel_hot_100_2023-11-12</td>\n",
       "      <td>Passport Questions &amp; Issues Megathread (2023)</td>\n",
       "      <td>NOTE: October 2023 **If the US Government has ...</td>\n",
       "      <td>2023-01-01 12:56:19</td>\n",
       "      <td>100t75r</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/100t7...</td>\n",
       "      <td>0.99</td>\n",
       "      <td>536.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>question</td>\n",
       "      <td>[SPRING BREAK RUSH HAS STARTED. AS OF TODAY PR...</td>\n",
       "      <td>Passport Questions &amp; Issues Megathread (2023) ...</td>\n",
       "      <td>passport question issue megathread 2023 note o...</td>\n",
       "      <td>the look that sol or use 9 give good u of stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reddit</td>\n",
       "      <td>travel_hot_100_2023-11-12</td>\n",
       "      <td>U.S. Department of State - \"Worldwide Caution\"</td>\n",
       "      <td>U.S. Department of State issued a new travel a...</td>\n",
       "      <td>2023-10-19 10:41:36</td>\n",
       "      <td>17bouw5</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/17bou...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>743.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>743.0</td>\n",
       "      <td>advice</td>\n",
       "      <td>[Yes. They are routinely issued when there is ...</td>\n",
       "      <td>U.S. Department of State - \"Worldwide Caution\"...</td>\n",
       "      <td>u s department of state worldwide caution u s ...</td>\n",
       "      <td>sept and a to potentially increase im s that i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reddit</td>\n",
       "      <td>travel_hot_100_2023-11-12</td>\n",
       "      <td>[Update] Jewelry stolen from luggage</td>\n",
       "      <td>First of all, I want to thank everyone who too...</td>\n",
       "      <td>2023-11-12 14:40:44</td>\n",
       "      <td>17tvwy3</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/17tvw...</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1296.0</td>\n",
       "      <td>advice</td>\n",
       "      <td>[While I’m glad you’re receiving full compensa...</td>\n",
       "      <td>[Update] Jewelry stolen from luggage First of ...</td>\n",
       "      <td>update jewelry steal from luggage first of all...</td>\n",
       "      <td>yourself our i airline i in and but sorry and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reddit</td>\n",
       "      <td>travel_hot_100_2023-11-12</td>\n",
       "      <td>Just me or is the US now far and away the most...</td>\n",
       "      <td>I’m American and everything from hotel prices/...</td>\n",
       "      <td>2023-11-12 15:27:10</td>\n",
       "      <td>17twyhu</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/17twy...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>881.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>881.0</td>\n",
       "      <td>question</td>\n",
       "      <td>[Of course. I live in San Diego and it blows m...</td>\n",
       "      <td>Just me or is the US now far and away the most...</td>\n",
       "      <td>just me or be the u now far and away the most ...</td>\n",
       "      <td>also tax as dollar eat find less of esp everyw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reddit</td>\n",
       "      <td>travel_hot_100_2023-11-12</td>\n",
       "      <td>We need to be more supportive of each other IR...</td>\n",
       "      <td>I’ve been traveling solo for a little bit and ...</td>\n",
       "      <td>2023-11-12 07:05:56</td>\n",
       "      <td>17tm34k</td>\n",
       "      <td>https://www.reddit.com/r/travel/comments/17tm3...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>383.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>advice</td>\n",
       "      <td>[I get tired of the \"I'm a traveller not a tou...</td>\n",
       "      <td>We need to be more supportive of each other IR...</td>\n",
       "      <td>we need to be more supportive of each other ir...</td>\n",
       "      <td>to no u while super me a the to and worldview ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4308</th>\n",
       "      <td>reddit</td>\n",
       "      <td>adventures_top_1000_2023-11-12</td>\n",
       "      <td>Best city and itinerary in Southeast Asia for ...</td>\n",
       "      <td>I (18M from USA) am looking to head to Southea...</td>\n",
       "      <td>2023-02-01 20:21:32</td>\n",
       "      <td>10rgjc9</td>\n",
       "      <td>https://www.reddit.com/r/adventures/comments/1...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[SE Asia is super easy to get around, so I rec...</td>\n",
       "      <td>Best city and itinerary in Southeast Asia for ...</td>\n",
       "      <td>best city and itinerary in southeast asia for ...</td>\n",
       "      <td>with thailand can understandably with north ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4309</th>\n",
       "      <td>reddit</td>\n",
       "      <td>adventures_top_1000_2023-11-12</td>\n",
       "      <td>Riverboarding Adventure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-26 13:58:54</td>\n",
       "      <td>10m3skn</td>\n",
       "      <td>https://youtu.be/qVti_oKTURY</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Riverboarding in the summer of 2022 on the Lo...</td>\n",
       "      <td>Riverboarding Adventure [Riverboarding in the ...</td>\n",
       "      <td>riverboarding adventure riverboarding in the s...</td>\n",
       "      <td>riverboarding adventure riverboarding in the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4310</th>\n",
       "      <td>reddit</td>\n",
       "      <td>adventures_top_1000_2023-11-12</td>\n",
       "      <td>Walkabout ideas?</td>\n",
       "      <td>Recent life events have left me needing a soci...</td>\n",
       "      <td>2023-01-22 07:34:05</td>\n",
       "      <td>10imakx</td>\n",
       "      <td>https://www.reddit.com/r/adventures/comments/1...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[I'm 42 and enjoy solo backcountry hiking as w...</td>\n",
       "      <td>Walkabout ideas? Recent life events have left ...</td>\n",
       "      <td>walkabout idea recent life event have leave me...</td>\n",
       "      <td>resupplies and this look m different and take ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4311</th>\n",
       "      <td>reddit</td>\n",
       "      <td>adventures_top_1000_2023-11-12</td>\n",
       "      <td>Riverside Camping on Algoma's Whiskey Lake Loop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-22 06:26:40</td>\n",
       "      <td>10iku8b</td>\n",
       "      <td>https://youtu.be/wkDCh09Zcfc</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Spent the night fishing, cooking and having s...</td>\n",
       "      <td>Riverside Camping on Algoma's Whiskey Lake Loo...</td>\n",
       "      <td>riverside camp on algoma s whiskey lake loop s...</td>\n",
       "      <td>riverside camp on algoma s whiskey lake loop s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4312</th>\n",
       "      <td>reddit</td>\n",
       "      <td>adventures_top_1000_2023-11-12</td>\n",
       "      <td>Winter Wild Camping at Kinder Down - Peak Dist...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-12-27 11:15:59</td>\n",
       "      <td>zwnguy</td>\n",
       "      <td>https://youtube.com/watch?v=__B_GP25Qpo&amp;featur...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Hope you don't mind me sharing my first prope...</td>\n",
       "      <td>Winter Wild Camping at Kinder Down - Peak Dist...</td>\n",
       "      <td>winter wild camp at kinder down peak district ...</td>\n",
       "      <td>at camp weather my here camp on wild bad out s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4312 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                        filename  \\\n",
       "0     reddit       travel_hot_100_2023-11-12   \n",
       "1     reddit       travel_hot_100_2023-11-12   \n",
       "2     reddit       travel_hot_100_2023-11-12   \n",
       "3     reddit       travel_hot_100_2023-11-12   \n",
       "4     reddit       travel_hot_100_2023-11-12   \n",
       "...      ...                             ...   \n",
       "4308  reddit  adventures_top_1000_2023-11-12   \n",
       "4309  reddit  adventures_top_1000_2023-11-12   \n",
       "4310  reddit  adventures_top_1000_2023-11-12   \n",
       "4311  reddit  adventures_top_1000_2023-11-12   \n",
       "4312  reddit  adventures_top_1000_2023-11-12   \n",
       "\n",
       "                                                  title  \\\n",
       "0         Passport Questions & Issues Megathread (2023)   \n",
       "1        U.S. Department of State - \"Worldwide Caution\"   \n",
       "2                  [Update] Jewelry stolen from luggage   \n",
       "3     Just me or is the US now far and away the most...   \n",
       "4     We need to be more supportive of each other IR...   \n",
       "...                                                 ...   \n",
       "4308  Best city and itinerary in Southeast Asia for ...   \n",
       "4309                            Riverboarding Adventure   \n",
       "4310                                   Walkabout ideas?   \n",
       "4311    Riverside Camping on Algoma's Whiskey Lake Loop   \n",
       "4312  Winter Wild Camping at Kinder Down - Peak Dist...   \n",
       "\n",
       "                                               selftext        creation_date  \\\n",
       "0     NOTE: October 2023 **If the US Government has ...  2023-01-01 12:56:19   \n",
       "1     U.S. Department of State issued a new travel a...  2023-10-19 10:41:36   \n",
       "2     First of all, I want to thank everyone who too...  2023-11-12 14:40:44   \n",
       "3     I’m American and everything from hotel prices/...  2023-11-12 15:27:10   \n",
       "4     I’ve been traveling solo for a little bit and ...  2023-11-12 07:05:56   \n",
       "...                                                 ...                  ...   \n",
       "4308  I (18M from USA) am looking to head to Southea...  2023-02-01 20:21:32   \n",
       "4309                                                NaN  2023-01-26 13:58:54   \n",
       "4310  Recent life events have left me needing a soci...  2023-01-22 07:34:05   \n",
       "4311                                                NaN  2023-01-22 06:26:40   \n",
       "4312                                                NaN  2022-12-27 11:15:59   \n",
       "\n",
       "           id                                                url  \\\n",
       "0     100t75r  https://www.reddit.com/r/travel/comments/100t7...   \n",
       "1     17bouw5  https://www.reddit.com/r/travel/comments/17bou...   \n",
       "2     17tvwy3  https://www.reddit.com/r/travel/comments/17tvw...   \n",
       "3     17twyhu  https://www.reddit.com/r/travel/comments/17twy...   \n",
       "4     17tm34k  https://www.reddit.com/r/travel/comments/17tm3...   \n",
       "...       ...                                                ...   \n",
       "4308  10rgjc9  https://www.reddit.com/r/adventures/comments/1...   \n",
       "4309  10m3skn                       https://youtu.be/qVti_oKTURY   \n",
       "4310  10imakx  https://www.reddit.com/r/adventures/comments/1...   \n",
       "4311  10iku8b                       https://youtu.be/wkDCh09Zcfc   \n",
       "4312   zwnguy  https://youtube.com/watch?v=__B_GP25Qpo&featur...   \n",
       "\n",
       "      upvote_ratio     ups  downs   score link_flair_css_class  \\\n",
       "0             0.99   536.0    0.0   536.0             question   \n",
       "1             0.94   743.0    0.0   743.0               advice   \n",
       "2             0.96  1296.0    0.0  1296.0               advice   \n",
       "3             0.88   881.0    0.0   881.0             question   \n",
       "4             0.91   383.0    0.0   383.0               advice   \n",
       "...            ...     ...    ...     ...                  ...   \n",
       "4308          1.00     1.0    0.0     1.0                  NaN   \n",
       "4309          1.00     1.0    0.0     1.0                  NaN   \n",
       "4310          1.00     1.0    0.0     1.0                  NaN   \n",
       "4311          1.00     1.0    0.0     1.0                  NaN   \n",
       "4312          1.00     1.0    0.0     1.0                  NaN   \n",
       "\n",
       "                                               comments  \\\n",
       "0     [SPRING BREAK RUSH HAS STARTED. AS OF TODAY PR...   \n",
       "1     [Yes. They are routinely issued when there is ...   \n",
       "2     [While I’m glad you’re receiving full compensa...   \n",
       "3     [Of course. I live in San Diego and it blows m...   \n",
       "4     [I get tired of the \"I'm a traveller not a tou...   \n",
       "...                                                 ...   \n",
       "4308  [SE Asia is super easy to get around, so I rec...   \n",
       "4309  [Riverboarding in the summer of 2022 on the Lo...   \n",
       "4310  [I'm 42 and enjoy solo backcountry hiking as w...   \n",
       "4311  [Spent the night fishing, cooking and having s...   \n",
       "4312  [Hope you don't mind me sharing my first prope...   \n",
       "\n",
       "                                               combined  \\\n",
       "0     Passport Questions & Issues Megathread (2023) ...   \n",
       "1     U.S. Department of State - \"Worldwide Caution\"...   \n",
       "2     [Update] Jewelry stolen from luggage First of ...   \n",
       "3     Just me or is the US now far and away the most...   \n",
       "4     We need to be more supportive of each other IR...   \n",
       "...                                                 ...   \n",
       "4308  Best city and itinerary in Southeast Asia for ...   \n",
       "4309  Riverboarding Adventure [Riverboarding in the ...   \n",
       "4310  Walkabout ideas? Recent life events have left ...   \n",
       "4311  Riverside Camping on Algoma's Whiskey Lake Loo...   \n",
       "4312  Winter Wild Camping at Kinder Down - Peak Dist...   \n",
       "\n",
       "                                            cause_clean  \\\n",
       "0     passport question issue megathread 2023 note o...   \n",
       "1     u s department of state worldwide caution u s ...   \n",
       "2     update jewelry steal from luggage first of all...   \n",
       "3     just me or be the u now far and away the most ...   \n",
       "4     we need to be more supportive of each other ir...   \n",
       "...                                                 ...   \n",
       "4308  best city and itinerary in southeast asia for ...   \n",
       "4309  riverboarding adventure riverboarding in the s...   \n",
       "4310  walkabout idea recent life event have leave me...   \n",
       "4311  riverside camp on algoma s whiskey lake loop s...   \n",
       "4312  winter wild camp at kinder down peak district ...   \n",
       "\n",
       "                                           bert_summary  \n",
       "0     the look that sol or use 9 give good u of stat...  \n",
       "1     sept and a to potentially increase im s that i...  \n",
       "2     yourself our i airline i in and but sorry and ...  \n",
       "3     also tax as dollar eat find less of esp everyw...  \n",
       "4     to no u while super me a the to and worldview ...  \n",
       "...                                                 ...  \n",
       "4308  with thailand can understandably with north ca...  \n",
       "4309  riverboarding adventure riverboarding in the s...  \n",
       "4310  resupplies and this look m different and take ...  \n",
       "4311  riverside camp on algoma s whiskey lake loop s...  \n",
       "4312  at camp weather my here camp on wild bad out s...  \n",
       "\n",
       "[4312 rows x 16 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuXr4Om_y9RI"
   },
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{output_dir}/processed_data.csv\", index=False, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "wyN_hNofRUth"
   },
   "outputs": [],
   "source": [
    "# df.to_csv(f\"{output_dir}/processed_data.csv\", index=False)\n",
    "# update appropriate title here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
